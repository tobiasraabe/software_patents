{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook replicates the final approach in my bachelor thesis. I cannot exactly replicate the findings but it more or less the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "\n",
    "ORIGINAL_RESULTS = [0.875, 0.875, 0.875, 0.771, 0.895,\n",
    "                    0.875, 0.543, 0.875, 0.448, 0.314]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/bh2007_classified', '/patents_catalogue_classified']\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(os.path.join('..', '..', 'bld', 'out', 'data', 'db_analysis.hdf')) as store:\n",
    "    print(store.keys())\n",
    "    bh_class = store.get('bh2007_classified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = bh_class.classification_manual\n",
    "x = bh_class.title.str.cat([bh_class.abstract, bh_class.description], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "nonsoftware       0.95      1.00      0.97        35\n",
      "   software       1.00      0.60      0.75         5\n",
      "\n",
      "avg / total       0.95      0.95      0.94        40\n",
      "\n",
      "[[35  0]\n",
      " [ 2  3]]\n",
      "0.724137931034\n",
      "------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "nonsoftware       0.97      1.00      0.99        35\n",
      "   software       1.00      0.80      0.89         5\n",
      "\n",
      "avg / total       0.98      0.97      0.97        40\n",
      "\n",
      "[[35  0]\n",
      " [ 1  4]]\n",
      "0.875\n",
      "------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "nonsoftware       0.97      1.00      0.99        35\n",
      "   software       1.00      0.80      0.89         5\n",
      "\n",
      "avg / total       0.98      0.97      0.97        40\n",
      "\n",
      "[[35  0]\n",
      " [ 1  4]]\n",
      "0.875\n",
      "------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "nonsoftware       0.97      0.97      0.97        35\n",
      "   software       0.80      0.80      0.80         5\n",
      "\n",
      "avg / total       0.95      0.95      0.95        40\n",
      "\n",
      "[[34  1]\n",
      " [ 1  4]]\n",
      "0.771428571429\n",
      "------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "nonsoftware       1.00      0.97      0.99        35\n",
      "   software       0.83      1.00      0.91         5\n",
      "\n",
      "avg / total       0.98      0.97      0.98        40\n",
      "\n",
      "[[34  1]\n",
      " [ 0  5]]\n",
      "0.894736842105\n",
      "------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "nonsoftware       0.97      0.97      0.97        35\n",
      "   software       0.80      0.80      0.80         5\n",
      "\n",
      "avg / total       0.95      0.95      0.95        40\n",
      "\n",
      "[[34  1]\n",
      " [ 1  4]]\n",
      "0.771428571429\n",
      "------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "nonsoftware       0.94      0.94      0.94        35\n",
      "   software       0.60      0.60      0.60         5\n",
      "\n",
      "avg / total       0.90      0.90      0.90        40\n",
      "\n",
      "[[33  2]\n",
      " [ 2  3]]\n",
      "0.542857142857\n",
      "------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "nonsoftware       0.97      1.00      0.99        35\n",
      "   software       1.00      0.80      0.89         5\n",
      "\n",
      "avg / total       0.98      0.97      0.97        40\n",
      "\n",
      "[[35  0]\n",
      " [ 1  4]]\n",
      "0.875\n",
      "------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "nonsoftware       0.94      0.97      0.96        35\n",
      "   software       0.75      0.60      0.67         5\n",
      "\n",
      "avg / total       0.92      0.93      0.92        40\n",
      "\n",
      "[[34  1]\n",
      " [ 2  3]]\n",
      "0.625\n",
      "------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "nonsoftware       0.91      0.91      0.91        35\n",
      "   software       0.40      0.40      0.40         5\n",
      "\n",
      "avg / total       0.85      0.85      0.85        40\n",
      "\n",
      "[[32  3]\n",
      " [ 3  2]]\n",
      "0.314285714286\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "                    ('vect', CountVectorizer(max_df=0.7, ngram_range=(1,4), analyzer='word')),\n",
    "                    ('sel', SelectKBest(chi2, k=200)),\n",
    "                    ('clf', RandomForestClassifier(n_estimators=5000, \n",
    "                            criterion='gini', max_features=0.3, n_jobs=-1)),\n",
    "                    ])\n",
    "    \n",
    "scores = []\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=10)\n",
    "for train_index, test_index in sss.split(x, y):\n",
    "    x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    classifier = text_clf.fit(x_train, y_train)\n",
    "    predicted = text_clf.predict(x_test)\n",
    "    \n",
    "    # Append scores to list\n",
    "    scores.append(cohen_kappa_score(y_test, predicted))\n",
    "\n",
    "#     print(metrics.classification_report(y_test, predicted, target_names=['nonsoftware', 'software']))\n",
    "#     print(metrics.confusion_matrix(y_test, predicted))\n",
    "#     print(cohen_kappa_score(y_test, predicted))\n",
    "#     print('------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 3 decimals\n\n(mismatch 50.0%)\n x: array([ 0.314,  0.448,  0.543,  0.771,  0.875,  0.875,  0.875,  0.875,\n        0.875,  0.895])\n y: array([ 0.314,  0.543,  0.625,  0.724,  0.771,  0.771,  0.875,  0.875,\n        0.875,  0.895])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-ebeb5acaff75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_array_almost_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mORIGINAL_RESULTS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\tobia\\AppData\\Local\\conda\\conda\\envs\\software_patents\\lib\\site-packages\\numpy\\testing\\utils.py\u001b[0m in \u001b[0;36massert_array_almost_equal\u001b[1;34m(x, y, decimal, err_msg, verbose)\u001b[0m\n\u001b[0;32m    960\u001b[0m     assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n\u001b[0;32m    961\u001b[0m              \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Arrays are not almost equal to %d decimals'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m              precision=decimal)\n\u001b[0m\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\tobia\\AppData\\Local\\conda\\conda\\envs\\software_patents\\lib\\site-packages\\numpy\\testing\\utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[1;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[0;32m    776\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m    777\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 778\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    779\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: \nArrays are not almost equal to 3 decimals\n\n(mismatch 50.0%)\n x: array([ 0.314,  0.448,  0.543,  0.771,  0.875,  0.875,  0.875,  0.875,\n        0.875,  0.895])\n y: array([ 0.314,  0.543,  0.625,  0.724,  0.771,  0.771,  0.875,  0.875,\n        0.875,  0.895])"
     ]
    }
   ],
   "source": [
    "npt.assert_array_almost_equal(ORIGINAL_RESULTS, sorted(scores), decimal=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npt.assert_almost_equal(np.mean(ORIGINAL_RESULTS), np.mean(scores), decimal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72688747731397452"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
