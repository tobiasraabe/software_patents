{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook creates a framework for future machine learning algorithms on the sample of Bessen and Hunt (2007) which is currently my only tagged patents corpora.\n",
    "\n",
    "The strucuture has to be a pipeline inspired by scikit-learn's pipelines which goes to all steps from text processing to the prediction. Of course, we implement the design with a randomized gridsearch to find the best parameters for the prediction model as well as cross-validation to overcome overfitting.\n",
    "\n",
    "The major problem is the sample size of only 400 patents where only about 40 are labelled manually as software patents. We have to alleviate this obstacle by good techniques or additional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "SEED = 123\n",
    "\n",
    "kappa_scorer = make_scorer(cohen_kappa_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/bh2007_classified', '/patents_catalogue_classified']\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(os.path.join('..', '..', 'bld', 'out', 'data', 'db_analysis.hdf')) as store:\n",
    "    print(store.keys())\n",
    "    bh_class = store.get('bh2007_classified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = bh_class.classification_manual\n",
    "x = bh_class.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator\n",
      "Pipeline(memory=None,\n",
      "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.84000000000000008, max_features=None,\n",
      "        min_df=1, ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words='english', strip_accents=None,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None)), ('classifier', MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))])\n",
      "Best score\n",
      "0.595276667391\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vectorizer = {\n",
    "    'vectorizer__stop_words': ['english', None],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'vectorizer__max_df': np.arange(0.8, 1, 0.04),\n",
    "#     'vectorizer__min_df': np.arange(0, 0.2, 0.04),\n",
    "#     'vectorizer__max_features': np.arange(0.8, 1.0, 0.04),\n",
    "}\n",
    "\n",
    "parameter_candidates = [\n",
    "    {\n",
    "        'classifier__alpha': [0.1, 0.5, 1, 2, 3],        \n",
    "    },\n",
    "    {\n",
    "        'classifier__n_estimators': [200, 500, 1000],\n",
    "        'classifier__max_features': np.arange(0.1, 1.1, 0.1),\n",
    "    },\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "\n",
    "for i, model in enumerate([\n",
    "        MultinomialNB(),\n",
    "#         RandomForestClassifier()\n",
    "    ]):\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer()),\n",
    "        ('classifier', model),\n",
    "    ])\n",
    "    \n",
    "    parameter_candidates[i].update(vectorizer)\n",
    "\n",
    "    clf = RandomizedSearchCV(pipeline, parameter_candidates[i], cv=3, scoring=kappa_scorer,\n",
    "                             random_state=SEED, n_jobs=-1, n_iter=10)\n",
    "    model_fit = clf.fit(x, y)\n",
    "\n",
    "    print('Best estimator')\n",
    "    print(model_fit.best_estimator_)\n",
    "    print('Best score')\n",
    "    print(model_fit.best_score_)\n",
    "    print('----------------------------------------------')\n",
    "    if i == 0:\n",
    "        best_model = model_fit\n",
    "    else:\n",
    "        best_model = model_fit if (model_fit.best_score_ > best_model.best_score_) else best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
