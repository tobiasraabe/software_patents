{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook creates a framework for future machine learning algorithms on the sample of Bessen and Hunt (2007) which is currently my only tagged patents corpora.\n",
    "\n",
    "The strucuture has to be a pipeline inspired by scikit-learn's pipelines which goes to all steps from text processing to the prediction. Of course, we implement the design with a randomized gridsearch to find the best parameters for the prediction model as well as cross-validation to overcome overfitting.\n",
    "\n",
    "The major problem is the sample size of only 400 patents where only about 40 are labelled manually as software patents. We have to alleviate this obstacle by good techniques or additional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import pickle\n",
    "import string\n",
    "import time\n",
    "\n",
    "from itertools import compress\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk import sent_tokenize\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from operator import itemgetter\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "\n",
    "SEED = 12345\n",
    "\n",
    "\n",
    "def timeit(func):\n",
    "    \"\"\"Simple timing decorator.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        delta = time.time() - start\n",
    "        return result, delta\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg\n",
    "\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        self.lower = lower\n",
    "        self.strip = strip\n",
    "        self.stopwords = stopwords or set(sw.words('english'))\n",
    "        self.punct = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(document):\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If stopword, ignore token and continue\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "                    \n",
    "                # If token number, ignore and continue\n",
    "                try:\n",
    "                    int(token)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                else:\n",
    "                    continue\n",
    "                # If token contains fig, ignore and continue\n",
    "                if 'fig' in token:\n",
    "                    continue\n",
    "                    \n",
    "                # If token has length one, ignore and continue\n",
    "                if len(token) == 1:\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "\n",
    "@timeit\n",
    "def build_and_evaluate(X, y, classifier=ExtraTreesClassifier,\n",
    "                       n_splits=2, test_size=0.2, seed=SEED,\n",
    "                       outpath=None, verbose=True, debug=False):\n",
    "\n",
    "    @timeit\n",
    "    def build(classifier, X, y=None):\n",
    "        \"\"\"Inner build function that builds a single model.\n",
    "        \"\"\"\n",
    "        if isinstance(classifier, type):\n",
    "            classifier = classifier()\n",
    "\n",
    "        model = Pipeline([\n",
    "            ('preprocessor', NLTKPreprocessor()),\n",
    "            ('vectorizer', TfidfVectorizer(\n",
    "                tokenizer=identity, preprocessor=None, lowercase=False, ngram_range=(1, 4),\n",
    "                min_df=0.05\n",
    "            )),\n",
    "            ('feature_selection', SelectFromModel(RandomForestClassifier(n_jobs=3), threshold='mean')),\n",
    "            ('classifier', classifier),\n",
    "        ])\n",
    "\n",
    "        model.fit(X, y)\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Label encode the targets\n",
    "    labels = LabelEncoder()\n",
    "    y = labels.fit_transform(y)\n",
    "\n",
    "    # Begin evaluation\n",
    "    if verbose:\n",
    "        print(\"Building for evaluation\")\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size,\n",
    "                                 random_state=seed)\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model, secs = build(classifier, X_train, y_train)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Evaluation model fit in {:0.3f} seconds\".format(secs))\n",
    "            print(\"Classification Report:\\n\")\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        if verbose:\n",
    "            print(clsr(y_test, y_pred, target_names=labels.classes_))\n",
    "            print('Kappa score: ', str(cohen_kappa_score(y_test, y_pred)))\n",
    "    \n",
    "    if not debug:\n",
    "        if verbose:\n",
    "            print(\"Building complete model and saving ...\")\n",
    "        model, secs = build(classifier, X, y)\n",
    "        model.labels_ = labels\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Complete model fit in {:0.3f} seconds\".format(secs))\n",
    "\n",
    "        if outpath:\n",
    "            with open(outpath, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "\n",
    "            print(\"Model written out to {}\".format(outpath))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def show_most_informative_features(model, text=None, n=20, outpath=None):\n",
    "    # Extract the vectorizer and the classifier from the pipeline\n",
    "    vectorizer = model.named_steps['vectorizer']\n",
    "    classifier = model.named_steps['classifier']\n",
    "    \n",
    "    try:\n",
    "        feature_selection = model.named_steps['feature_selection']\n",
    "    except AttributeError:\n",
    "        raise\n",
    "\n",
    "    # Check to make sure that we can perform this computation\n",
    "    if not (hasattr(classifier, 'coef_') | hasattr(classifier, 'feature_importances_')):\n",
    "        raise TypeError(\n",
    "            \"Cannot compute most informative features on {}.\".format(\n",
    "                classifier.__class__.__name__\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if text is not None:\n",
    "        # Compute the coefficients for the text\n",
    "        tvec = model.transform([text]).toarray()\n",
    "    else:\n",
    "        # Otherwise simply use the coefficients\n",
    "        try:\n",
    "            tvec = classifier.coef_\n",
    "        except AttributeError:\n",
    "            tvec = classifier.feature_importances_\n",
    "            \n",
    "    # Determine the names from features which pass feature selection\n",
    "    names = list(compress(vectorizer.get_feature_names(), feature_selection.get_support()))\n",
    "    # Zip the feature names with the coefs and sort\n",
    "    try:\n",
    "        coefs = sorted(\n",
    "            zip(tvec[0], names),\n",
    "            key=itemgetter(0), reverse=True\n",
    "        )\n",
    "    except TypeError:\n",
    "        coefs = sorted(\n",
    "            zip(tvec, names),\n",
    "            key=itemgetter(0), reverse=True\n",
    "        )\n",
    "\n",
    "    # Get the top n and bottom n coef, name pairs\n",
    "    topn = zip(coefs[:n], coefs[:-(n + 1):-1])\n",
    "\n",
    "    # Create the output string to return\n",
    "    output = []\n",
    "\n",
    "    # If text, add the predicted value to the output.\n",
    "    if text is not None:\n",
    "        output.append(\"\\\"{}\\\"\".format(text))\n",
    "        output.append(\n",
    "            \"Classified as: {}\".format(model.predict([text]))\n",
    "        )\n",
    "        output.append(\"\")\n",
    "\n",
    "    # Create two columns with most negative and most positive features.\n",
    "    for (cp, fnp), (cn, fnn) in topn:\n",
    "        output.append(\n",
    "            \"{:0.4f}{: >20}\\t\\t{:0.4f}{: >20}\".format(\n",
    "                cp, fnp, cn, fnn\n",
    "            )\n",
    "        )\n",
    "\n",
    "    table = \"\\n\".join(output)\n",
    "\n",
    "    if outpath:\n",
    "        with open(outpath, 'w') as file:\n",
    "            file.write(table)\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('preprocessor', NLTKPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer(\n",
    "        tokenizer=identity, preprocessor=None, lowercase=False, ngram_range=(1, 4),\n",
    "        min_df=0.05\n",
    "    )),\n",
    "    ('feature_selection', SelectFromModel(RandomForestClassifier(n_jobs=3), threshold='mean')),\n",
    "    ('classifier', ExtraTreesClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "isinstance() arg 2 must be a type or tuple of types",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ceb198cb2b6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: isinstance() arg 2 must be a type or tuple of types"
     ]
    }
   ],
   "source": [
    "isinstance(Pipeline, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/bh2007', '/patents_catalogue']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "with pd.HDFStore(os.path.join('..', '..', 'bld', 'out', 'data', 'db_data_preparation.hdf')) as store:\n",
    "    print(store.keys())\n",
    "    bh_class = store.get('bh2007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = bh_class.classification_manual.map({False: 'Non-Software', True: 'Software'})\n",
    "x = bh_class.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation\n",
      "Evaluation model fit in 217.364 seconds\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-Software       0.95      1.00      0.97        69\n",
      "    Software       1.00      0.64      0.78        11\n",
      "\n",
      " avg / total       0.95      0.95      0.95        80\n",
      "\n",
      "Kappa score:  0.751166407465\n",
      "Evaluation model fit in 205.573 seconds\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-Software       0.91      1.00      0.95        69\n",
      "    Software       1.00      0.36      0.53        11\n",
      "\n",
      " avg / total       0.92      0.91      0.89        80\n",
      "\n",
      "Kappa score:  0.496402877698\n"
     ]
    }
   ],
   "source": [
    "PATH = 'model.pkl'\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=1000, n_jobs=3, max_features=0.3)\n",
    "model = build_and_evaluate(x, y, classifier=clf, outpath=PATH, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3192,), 169)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].named_steps['feature_selection'].get_support().shape, model[0].named_steps['feature_selection'].get_support().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0665            computer\t\t0.0003            elongate\n",
      "0.0603            software\t\t0.0005     invention apply\n",
      "0.0552             program\t\t0.0005          extraction\n",
      "0.0411             command\t\t0.0006          protection\n",
      "0.0406             whether\t\t0.0006             loading\n",
      "0.0293           implement\t\t0.0006          downwardly\n",
      "0.0267     invention block\t\t0.0007             respond\n",
      "0.0226            data use\t\t0.0008               press\n",
      "0.0179                time\t\t0.0008               touch\n",
      "0.0159        store memory\t\t0.0009             defined\n",
      "0.0153           determine\t\t0.0010                pull\n",
      "0.0147      microprocessor\t\t0.0010       consideration\n",
      "0.0139          system use\t\t0.0011        manufacturer\n",
      "0.0135             display\t\t0.0011                 cap\n",
      "0.0131              memory\t\t0.0012           fabricate\n",
      "0.0129              detect\t\t0.0013         engineering\n",
      "0.0126invention block diagram\t\t0.0013              induce\n",
      "0.0120          determines\t\t0.0013            energize\n",
      "0.0113            hardware\t\t0.0013               yield\n",
      "0.0112           interface\t\t0.0013             abandon\n"
     ]
    }
   ],
   "source": [
    "print(show_most_informative_features(model[0], n=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
